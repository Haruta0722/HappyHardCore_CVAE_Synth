import tensorflow as tf
import numpy as np
import os
import json
from datetime import datetime
from model import TimeWiseCVAE, TIME_LENGTH, LATENT_STEPS, LATENT_DIM
from create_datasets import make_dataset_from_synth_csv


class TrainingState:
    """
    è¨“ç·´çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, checkpoint_dir="checkpoints"):
        self.checkpoint_dir = checkpoint_dir
        self.state_file = os.path.join(checkpoint_dir, "training_state.json")
        os.makedirs(checkpoint_dir, exist_ok=True)

    def save_state(self, epoch, step, best_loss, history):
        """
        è¨“ç·´çŠ¶æ…‹ã‚’ä¿å­˜
        """
        state = {
            "epoch": int(epoch),
            "step": int(step),
            "best_loss": float(best_loss),
            "history": {
                k: [float(v) for v in vals] for k, vals in history.items()
            },
            "timestamp": datetime.now().isoformat(),
        }

        with open(self.state_file, "w") as f:
            json.dump(state, f, indent=2)

        print(f"  âœ“ è¨“ç·´çŠ¶æ…‹ã‚’ä¿å­˜: {self.state_file}")

    def load_state(self):
        """
        è¨“ç·´çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿
        """
        if not os.path.exists(self.state_file):
            return None

        with open(self.state_file, "r") as f:
            state = json.load(f)

        print(f"  âœ“ è¨“ç·´çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿: {self.state_file}")
        print(f"    å‰å›ã®ã‚¨ãƒãƒƒã‚¯: {state['epoch']}")
        print(f"    å‰å›ã®ã‚¹ãƒ†ãƒƒãƒ—: {state['step']}")
        print(f"    ãƒ™ã‚¹ãƒˆæå¤±: {state['best_loss']:.6f}")

        return state

    def get_latest_checkpoint(self):
        """
        æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
        """
        checkpoints = [
            f
            for f in os.listdir(self.checkpoint_dir)
            if f.startswith("epoch_") and f.endswith(".weights.h5")
        ]

        if not checkpoints:
            return None

        # ã‚¨ãƒãƒƒã‚¯ç•ªå·ã§ã‚½ãƒ¼ãƒˆ
        checkpoints.sort(key=lambda x: int(x.split("_")[1].split(".")[0]))
        latest = os.path.join(self.checkpoint_dir, checkpoints[-1])

        print(f"  âœ“ æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {latest}")
        return latest


class ProgressCallback(tf.keras.callbacks.Callback):
    """
    å­¦ç¿’é€²æ—ã‚’è©³ç´°ã«è¡¨ç¤ºã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """

    def __init__(self, steps_per_epoch):
        super().__init__()
        self.steps_per_epoch = steps_per_epoch

    def on_epoch_begin(self, epoch, logs=None):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch + 1}")
        print(f"{'='*60}")

    def on_epoch_end(self, epoch, logs=None):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch + 1} å®Œäº†")
        print(f"{'='*60}")
        print(f"  loss:       {logs.get('loss', 0):.6f}")
        print(f"  recon:      {logs.get('recon', 0):.6f}")
        print(f"  stft:       {logs.get('stft', 0):.6f}")
        print(f"  mel:        {logs.get('mel', 0):.6f}")
        print(f"  kl:         {logs.get('kl', 0):.6f}")
        print(f"  kl_weight:  {logs.get('kl_weight', 0):.6f}")
        print(f"  z_std_ema:  {logs.get('z_std_ema', 0):.6f}")
        print(f"  grad_norm:  {logs.get('grad_norm', 0):.6f}")

        # è­¦å‘Šãƒã‚§ãƒƒã‚¯
        if logs.get("z_std_ema", 1.0) < 0.05:
            print(f"\nâš ï¸  WARNING: Posterior Collapse ã®å…†å€™")


class GenerationTestCallback(tf.keras.callbacks.Callback):
    """
    å®šæœŸçš„ã«éŸ³å£°ã‚’ç”Ÿæˆã—ã¦ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """

    def __init__(self, test_interval=10, output_dir="generation_tests"):
        super().__init__()
        self.test_interval = test_interval
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % self.test_interval != 0:
            return

        print(f"\n[ç”Ÿæˆãƒ†ã‚¹ãƒˆ] Epoch {epoch + 1}")

        import soundfile as sf

        # ãƒ†ã‚¹ãƒˆæ¡ä»¶
        test_cases = [
            (60, (1, 0, 0), "screech"),
            (60, (0, 1, 0), "acid"),
            (60, (0, 0, 1), "pluck"),
        ]

        for pitch, cond, name in test_cases:
            pitch_norm = (pitch - 36.0) / 35.0
            cond_vector = tf.constant([[pitch_norm, *cond]], dtype=tf.float32)

            # ãƒ©ãƒ³ãƒ€ãƒ ãªæ½œåœ¨å¤‰æ•°
            z = tf.random.normal((1, LATENT_STEPS, LATENT_DIM), stddev=0.7)

            # ç”Ÿæˆ
            x_hat = self.model.decoder([z, cond_vector])
            x_hat = tf.squeeze(x_hat).numpy()

            # æ­£è¦åŒ–
            max_val = np.max(np.abs(x_hat))
            if max_val > 1e-6:
                x_hat = x_hat / max_val * 0.95

            # ä¿å­˜
            filename = os.path.join(
                self.output_dir, f"epoch_{epoch+1:03d}_{name}.wav"
            )
            sf.write(filename, x_hat, samplerate=48000)

        print(
            f"  âœ“ ãƒ†ã‚¹ãƒˆéŸ³å£°ã‚’ç”Ÿæˆ: {self.output_dir}/epoch_{epoch+1:03d}_*.wav"
        )


class CollapseDetectionCallback(tf.keras.callbacks.Callback):
    """
    Posterior Collapse ã‚’æ¤œå‡ºã—ã¦è­¦å‘Š
    """

    def __init__(self, threshold=0.05, patience=5):
        super().__init__()
        self.threshold = threshold
        self.patience = patience
        self.low_std_count = 0

    def on_epoch_end(self, epoch, logs=None):
        z_std = logs.get("z_std_ema", 1.0)

        if z_std < self.threshold:
            self.low_std_count += 1
            print(f"\nâš ï¸  WARNING: z_std_ema={z_std:.4f} < {self.threshold}")
            print(
                f"   Posterior Collapse ã®å…†å€™ ({self.low_std_count}/{self.patience})"
            )

            if self.low_std_count >= self.patience:
                print("\nğŸš¨ CRITICAL: Posterior Collapse æ¤œå‡ºï¼")
                print("   æ¨å¥¨å¯¾ç­–:")
                print("   1. KLé‡ã¿ã‚’1/10ã«æ¸›ã‚‰ã™")
                print("   2. Free Bitsã‚’å¢—ã‚„ã™ (0.8 â†’ 1.2)")
                print("   3. å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹")
        else:
            self.low_std_count = 0


def train_model(
    dataset_path="dataset.csv",
    batch_size=16,
    epochs=200,
    initial_epoch=0,
    checkpoint_dir="checkpoints",
    resume=True,
    save_interval=5,
):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´

    Args:
        dataset_path: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®CSVãƒ‘ã‚¹
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        epochs: ç·ã‚¨ãƒãƒƒã‚¯æ•°
        initial_epoch: é–‹å§‹ã‚¨ãƒãƒƒã‚¯ï¼ˆé€šå¸¸ã¯0ã€å†é–‹æ™‚ã¯è‡ªå‹•è¨­å®šï¼‰
        checkpoint_dir: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        resume: True ã®å ´åˆã€å‰å›ã®å­¦ç¿’ã‹ã‚‰å†é–‹
        save_interval: ä½•ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ä¿å­˜ã™ã‚‹ã‹
    """
    print("=" * 60)
    print("è¡¨ç¾åŠ›è±Šã‹ãªéŸ³è‰²ãƒ¢ãƒ‡ãƒ« è¨“ç·´é–‹å§‹")
    print("=" * 60)

    # è¨“ç·´çŠ¶æ…‹ç®¡ç†
    training_state = TrainingState(checkpoint_dir)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
    print("\n[1] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ä¸­...")
    dataset = make_dataset_from_synth_csv(dataset_path, batch_size=batch_size)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’å–å¾—
    import pandas as pd

    df = pd.read_csv(dataset_path)
    total_samples = len(df)
    steps_per_epoch = total_samples // batch_size

    print(f"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples}")
    print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    print(f"  ã‚¹ãƒ†ãƒƒãƒ—/ã‚¨ãƒãƒƒã‚¯: {steps_per_epoch}")

    dataset = dataset.repeat()

    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    print("\n[2] ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ä¸­...")
    model = TimeWiseCVAE(steps_per_epoch=steps_per_epoch)

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ“ãƒ«ãƒ‰
    x_dummy, cond_dummy = next(iter(dataset))
    _ = model((x_dummy, cond_dummy), training=False)

    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
    model.compile(optimizer=optimizer)

    print(f"  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: {model.encoder.count_params():,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    print(f"  ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼: {model.decoder.count_params():,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    print(f"  åˆè¨ˆ: {model.count_params():,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")

    # å­¦ç¿’ã®å†é–‹å‡¦ç†
    best_loss = float("inf")
    history = {
        "loss": [],
        "recon": [],
        "stft": [],
        "mel": [],
        "kl": [],
        "kl_weight": [],
        "z_std_ema": [],
        "grad_norm": [],
    }

    if resume:
        print("\n[3] å‰å›ã®å­¦ç¿’çŠ¶æ…‹ã‚’ç¢ºèªä¸­...")
        state = training_state.load_state()

        if state is not None:
            initial_epoch = state["epoch"]
            best_loss = state["best_loss"]
            history = state["history"]

            # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
            latest_checkpoint = training_state.get_latest_checkpoint()
            if latest_checkpoint:
                model.load_weights(latest_checkpoint)
                print(f"  âœ“ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                print(f"\n  â†’ Epoch {initial_epoch + 1} ã‹ã‚‰å†é–‹ã—ã¾ã™")
            else:
                print(f"  âš ï¸  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                print(f"  â†’ Epoch 1 ã‹ã‚‰æ–°è¦ã«é–‹å§‹ã—ã¾ã™")
                initial_epoch = 0
        else:
            print(f"  è¨“ç·´çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print(f"  â†’ æ–°è¦ã«è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™")
    else:
        print("\n[3] æ–°è¦è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™")

    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
    print("\n[4] ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šä¸­...")
    callbacks = [
        # é€²æ—è¡¨ç¤º
        ProgressCallback(steps_per_epoch),
        # Collapseæ¤œå‡º
        CollapseDetectionCallback(threshold=0.05, patience=5),
        # å®šæœŸçš„ãªç”Ÿæˆãƒ†ã‚¹ãƒˆ
        GenerationTestCallback(test_interval=10),
        # å®šæœŸçš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        tf.keras.callbacks.LambdaCallback(
            on_epoch_end=lambda epoch, logs: (
                (
                    model.save_weights(
                        os.path.join(
                            checkpoint_dir, f"epoch_{epoch+1:03d}.weights.h5"
                        )
                    )
                    if (epoch + 1) % save_interval == 0
                    else None
                ),
                (
                    training_state.save_state(
                        epoch + 1,
                        (epoch + 1) * steps_per_epoch,
                        logs.get("loss", float("inf")),
                        {
                            k: history[k] + [logs.get(k, 0)]
                            for k in history.keys()
                        },
                    )
                    if (epoch + 1) % save_interval == 0
                    else None
                ),
                (
                    print(f"\n  âœ“ Epoch {epoch+1} ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                    if (epoch + 1) % save_interval == 0
                    else None
                ),
            )
        ),
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(checkpoint_dir, "best_model.weights.h5"),
            monitor="loss",
            mode="min",
            save_best_only=True,
            save_weights_only=True,
            verbose=1,
        ),
        # å­¦ç¿’ç‡å‰Šæ¸›
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="loss", factor=0.5, patience=15, min_lr=1e-6, verbose=1
        ),
        # CSVãƒ­ã‚°
        tf.keras.callbacks.CSVLogger(
            os.path.join(checkpoint_dir, "training_log.csv"), append=True
        ),
    ]

    # å­¦ç¿’æˆ¦ç•¥ã®è¡¨ç¤º
    print("\n[5] å­¦ç¿’æˆ¦ç•¥:")
    print(f"  é–‹å§‹ã‚¨ãƒãƒƒã‚¯: {initial_epoch + 1}")
    print(f"  çµ‚äº†ã‚¨ãƒãƒƒã‚¯: {epochs}")
    print(f"  KL Warmup: {model.kl_warmup_epochs} ã‚¨ãƒãƒƒã‚¯")
    print(f"  KL Rampup: {model.kl_rampup_epochs} ã‚¨ãƒãƒƒã‚¯")
    print(f"  KL Target: {model.kl_target}")
    print(f"  Free Bits: {model.free_bits}")
    print(f"  ä¿å­˜é–“éš”: {save_interval} ã‚¨ãƒãƒƒã‚¯ã”ã¨")

    # å­¦ç¿’é–‹å§‹
    print("\n[6] å­¦ç¿’é–‹å§‹...")
    print("=" * 60)

    try:
        history_obj = model.fit(
            dataset,
            epochs=epochs,
            initial_epoch=initial_epoch,
            steps_per_epoch=steps_per_epoch,
            callbacks=callbacks,
            verbose=1,
        )

        # æœ€çµ‚çŠ¶æ…‹ã‚’ä¿å­˜
        final_logs = history_obj.history
        for k in history.keys():
            if k in final_logs:
                history[k].extend(final_logs[k])

        training_state.save_state(
            epochs,
            epochs * steps_per_epoch,
            min(final_logs.get("loss", [float("inf")])),
            history,
        )

        print("\n" + "=" * 60)
        print("è¨“ç·´å®Œäº†ï¼")
        print("=" * 60)
        print(f"  æœ€çµ‚ loss: {final_logs['loss'][-1]:.6f}")
        print(f"  æœ€çµ‚ z_std_ema: {final_logs['z_std_ema'][-1]:.6f}")
        print(f"  æœ€çµ‚ kl_weight: {final_logs['kl_weight'][-1]:.6f}")

    except KeyboardInterrupt:
        print("\n\n" + "=" * 60)
        print("è¨“ç·´ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        print("=" * 60)
        print("æ¬¡å›å®Ÿè¡Œæ™‚ã« resume=True ã§å†é–‹ã§ãã¾ã™")

        # ä¸­æ–­æ™‚ã®çŠ¶æ…‹ã‚’ä¿å­˜
        current_epoch = model.optimizer.iterations.numpy() // steps_per_epoch
        training_state.save_state(
            current_epoch,
            model.optimizer.iterations.numpy(),
            best_loss,
            history,
        )

        # ä¸­æ–­æ™‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        interrupt_path = os.path.join(checkpoint_dir, "interrupted.weights.h5")
        model.save_weights(interrupt_path)
        print(f"  âœ“ ä¸­æ–­æ™‚ã®é‡ã¿ã‚’ä¿å­˜: {interrupt_path}")

    return model, history


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="è¡¨ç¾åŠ›è±Šã‹ãªéŸ³è‰²ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´")
    parser.add_argument(
        "--dataset",
        type=str,
        default="dataset.csv",
        help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®CSVãƒ‘ã‚¹",
    )
    parser.add_argument(
        "--batch-size", type=int, default=16, help="ãƒãƒƒãƒã‚µã‚¤ã‚º"
    )
    parser.add_argument("--epochs", type=int, default=200, help="ç·ã‚¨ãƒãƒƒã‚¯æ•°")
    parser.add_argument(
        "--checkpoint-dir",
        type=str,
        default="checkpoints",
        help="ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª",
    )
    parser.add_argument(
        "--no-resume",
        action="store_true",
        help="å‰å›ã®å­¦ç¿’ã‚’ç„¡è¦–ã—ã¦æ–°è¦ã«é–‹å§‹",
    )
    parser.add_argument(
        "--save-interval",
        type=int,
        default=5,
        help="ä½•ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ä¿å­˜ã™ã‚‹ã‹",
    )

    args = parser.parse_args()

    print("\nè¨­å®š:")
    print(f"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {args.dataset}")
    print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {args.batch_size}")
    print(f"  ç·ã‚¨ãƒãƒƒã‚¯æ•°: {args.epochs}")
    print(f"  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆDir: {args.checkpoint_dir}")
    print(f"  å†é–‹: {not args.no_resume}")
    print(f"  ä¿å­˜é–“éš”: {args.save_interval} ã‚¨ãƒãƒƒã‚¯")

    model, history = train_model(
        dataset_path=args.dataset,
        batch_size=args.batch_size,
        epochs=args.epochs,
        checkpoint_dir=args.checkpoint_dir,
        resume=not args.no_resume,
        save_interval=args.save_interval,
    )
