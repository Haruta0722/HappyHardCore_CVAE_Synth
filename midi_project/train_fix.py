import tensorflow as tf
from model import (
    TimeWiseCVAE,
    TIME_LENGTH,
    LATENT_STEPS,
    LATENT_DIM,
    recon_weight,
    STFT_weight,
    mel_weight,
    diff_weight,
)  # å¼·åŠ›ãªæ¡ä»¶ä»˜ã‘ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import numpy as np
from create_datasets import make_dataset_from_synth_csv


class AntiCollapseStrongCVAE(TimeWiseCVAE):
    """
    å¼·åŠ›ãªæ¡ä»¶ä»˜ã‘ + Posterior Collapseå¯¾ç­–ç‰ˆCVAE
    freq_featã¯ä½¿ç”¨ã—ãªã„
    """

    def __init__(self, steps_per_epoch=87, *args, **kwargs):
        super().__init__(steps_per_epoch=steps_per_epoch, *args, **kwargs)
        # è¦ªã‚¯ãƒ©ã‚¹ã§æ—¢ã«è¨­å®šæ¸ˆã¿

    def add_gaussian_noise_to_z(self, z, noise_scale=0.1):
        """
        å­¦ç¿’ä¸­ã«zã«ãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã¦ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’é ‘å¼·ã«ã™ã‚‹
        """
        noise = tf.random.normal(tf.shape(z), stddev=noise_scale)
        return z + noise

    def train_step(self, data):
        x, cond = data

        with tf.GradientTape() as tape:
            z_mean, z_logvar = self.encoder([x, cond])
            z = self.sample_z(z_mean, z_logvar)

            # â˜…å¯¾ç­–1: å­¦ç¿’ä¸­ã«zã«ãƒã‚¤ã‚ºã‚’è¿½åŠ 
            # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãŒzã®å°ã•ãªå¤‰åŒ–ã«ã‚‚å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
            z_noisy = self.add_gaussian_noise_to_z(z, noise_scale=0.05)

            # â˜…é‡è¦: freq_featã¯ä½¿ã‚ãªã„ï¼
            # å¼·åŠ›ãªæ¡ä»¶ä»˜ã‘ãƒ¢ãƒ‡ãƒ«ã¯æ¡ä»¶ãƒ™ã‚¯ãƒˆãƒ«ã¨zã ã‘ã§ç”Ÿæˆ
            x_hat = self.decoder([z_noisy, cond])

            x_hat = x_hat[:, :TIME_LENGTH, :]

            x_target = tf.squeeze(x, axis=-1)
            x_hat_sq = tf.squeeze(x_hat, axis=-1)

            # æå¤±è¨ˆç®—
            recon = tf.reduce_mean(tf.square(x_target - x_hat_sq))

            # â˜…å¯¾ç­–2: Free Bits KL
            kl_free_bits = self.compute_free_bits_kl(z_mean, z_logvar)

            # é€šå¸¸ã®KLã‚‚è¨ˆç®—ï¼ˆç›£è¦–ç”¨ï¼‰
            kl_standard = -0.5 * tf.reduce_mean(
                1 + z_logvar - tf.square(z_mean) - tf.exp(z_logvar)
            )

            from loss import Loss

            stft_loss, mel_loss, diff_loss = Loss(
                x_target, x_hat_sq, fft_size=2048, hop_size=512
            )

            # â˜…å¯¾ç­–3: æ®µéšçš„ãªKLé‡ã¿
            kl_weight = self.compute_kl_weight()

            loss = (
                recon * recon_weight  # 5.0
                + stft_loss * STFT_weight  # 15.0
                + mel_loss * mel_weight  # 10.0
                + diff_loss * diff_weight  # 3.0
                + kl_free_bits * kl_weight
            )

        grads = tape.gradient(loss, self.trainable_variables)
        grads, grad_norm = tf.clip_by_global_norm(grads, 5.0)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # â˜…å¯¾ç­–4: zã®æ´»ç”¨åº¦ã‚’ç›£è¦–
        z_std = tf.reduce_mean(tf.math.reduce_std(z_mean, axis=1))
        # æŒ‡æ•°ç§»å‹•å¹³å‡ã§å¹³æ»‘åŒ–
        self.z_std_ema.assign(0.99 * self.z_std_ema + 0.01 * z_std)

        # â˜…å¯¾ç­–5: è­¦å‘Šã‚·ã‚¹ãƒ†ãƒ 
        should_reduce_kl = tf.cond(
            self.z_std_ema < 0.05, lambda: True, lambda: False
        )

        return {
            "loss": loss,
            "recon": recon,
            "stft": stft_loss,
            "mel": mel_loss,
            "diff": diff_loss,
            "kl_standard": kl_standard,
            "kl_free_bits": kl_free_bits,
            "kl_weight": kl_weight,
            "z_std": z_std,
            "z_std_ema": self.z_std_ema,
            "grad_norm": grad_norm,
            "collapse_warning": tf.cast(should_reduce_kl, tf.float32),
        }

    def sample_z(self, z_mean, z_logvar):
        """
        Reparameterization trick
        """
        eps = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_logvar) * eps


# ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯: Collapseæ¤œå‡º
class CollapseDetectionCallback(tf.keras.callbacks.Callback):
    """
    å­¦ç¿’ä¸­ã«Posterior Collapseã‚’æ¤œå‡ºã—ã¦è­¦å‘Š
    """

    def __init__(self, threshold=0.05, patience=5):
        super().__init__()
        self.threshold = threshold
        self.patience = patience
        self.low_std_count = 0

    def on_epoch_end(self, epoch, logs=None):
        z_std = logs.get("z_std_ema", 1.0)

        if z_std < self.threshold:
            self.low_std_count += 1
            print(f"\nâš ï¸  WARNING: z_std={z_std:.4f} < {self.threshold}")
            print(
                f"   Posterior Collapse ã®å…†å€™ ({self.low_std_count}/{self.patience})"
            )

            if self.low_std_count >= self.patience:
                print("\nğŸš¨ CRITICAL: Posterior Collapse æ¤œå‡ºï¼")
                print("   æ¨å¥¨å¯¾ç­–:")
                print("   1. KLé‡ã¿ã‚’1/10ã«æ¸›ã‚‰ã™")
                print("   2. Free Bitsã‚’å¢—ã‚„ã™ (0.8 â†’ 1.2)")
                print("   3. å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹")
                print("   4. ã‚ˆã‚Šå¤šãã®Warmupã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½¿ã†\n")
        else:
            self.low_std_count = 0
            if epoch % 10 == 0:
                print(f"âœ“ z_std={z_std:.4f} - æ½œåœ¨å¤‰æ•°ã¯å¥å…¨ã§ã™")


class ConditionMonitorCallback(tf.keras.callbacks.Callback):
    """
    æ¡ä»¶ãƒ™ã‚¯ãƒˆãƒ«ã®åŠ¹æœã‚’ç›£è¦–
    å®šæœŸçš„ã«ç•°ãªã‚‹æ¡ä»¶ã§ç”Ÿæˆã—ã¦ä¿å­˜
    """

    def __init__(self, test_pitch=60, check_every=10):
        super().__init__()
        self.test_pitch = test_pitch
        self.check_every = check_every

    def on_epoch_end(self, epoch, logs=None):
        if epoch % self.check_every != 0:
            return

        print(f"\n[Epoch {epoch}] æ¡ä»¶åˆ¥ç”Ÿæˆãƒ†ã‚¹ãƒˆ...")

        import soundfile as sf

        pitch_norm = (self.test_pitch - 36.0) / 35.0
        conditions = {
            "screech": (1, 0, 0),
            "acid": (0, 1, 0),
            "pluck": (0, 0, 1),
        }

        for name, cond in conditions.items():
            cond_vector = tf.constant([[pitch_norm, *cond]], dtype=tf.float32)

            # ãƒ©ãƒ³ãƒ€ãƒ ãªzã§ç”Ÿæˆ
            z = tf.random.normal((1, LATENT_STEPS, LATENT_DIM), stddev=0.5)

            x_hat = self.model.decoder([z, cond_vector])
            x_hat = tf.squeeze(x_hat).numpy()

            # æ­£è¦åŒ–
            max_val = np.max(np.abs(x_hat))
            if max_val > 1e-6:
                x_hat = x_hat / max_val * 0.95

            filename = f"monitor/epoch_{epoch:03d}_{name}.wav"
            sf.write(filename, x_hat, samplerate=48000)

        print(f"  âœ“ ä¿å­˜: monitor/epoch_{epoch:03d}_*.wav")


# å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
def train_with_strong_conditioning(batch_size=16, epochs=200):
    """
    å¼·åŠ›ãªæ¡ä»¶ä»˜ã‘ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’
    """
    print("=" * 60)
    print("å¼·åŠ›ãªæ¡ä»¶ä»˜ã‘ãƒ¢ãƒ‡ãƒ« å­¦ç¿’é–‹å§‹")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\n[1] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...")
    dataset = make_dataset_from_synth_csv("dataset.csv", batch_size=batch_size)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨ˆç®—
    # dataset.csvã®è¡Œæ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„
    # ä¾‹: 348è¡Œã®ãƒ‡ãƒ¼ã‚¿ã€batch_size=16 â†’ steps_per_epoch = 348//16 = 21

    # â˜…é‡è¦: å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦å¤‰æ›´
    import pandas as pd

    df = pd.read_csv("dataset.csv")
    total_samples = len(df)
    steps_per_epoch = total_samples // batch_size

    print(f"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples}")
    print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    print(f"  ã‚¹ãƒ†ãƒƒãƒ—/ã‚¨ãƒãƒƒã‚¯: {steps_per_epoch}")

    dataset = dataset.repeat()

    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    print("\n[2] ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ä¸­...")
    model = AntiCollapseStrongCVAE(steps_per_epoch=steps_per_epoch)

    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆbuildï¼‰
    x_dummy, cond_dummy = next(iter(dataset))
    _ = model((x_dummy, cond_dummy), training=False)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))

    print(f"  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: {model.encoder.count_params():,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    print(f"  ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼: {model.decoder.count_params():,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    print(f"  åˆè¨ˆ: {model.count_params():,} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")

    # å­¦ç¿’æˆ¦ç•¥ã®è¡¨ç¤º
    print("\n[3] å­¦ç¿’æˆ¦ç•¥:")
    print(
        f"  KL Warmup: {model.kl_warmup_epochs} ã‚¨ãƒãƒƒã‚¯ ({model.kl_warmup_steps} ã‚¹ãƒ†ãƒƒãƒ—)"
    )
    print(
        f"  KL Rampup: {model.kl_rampup_epochs} ã‚¨ãƒãƒƒã‚¯ ({model.kl_rampup_steps} ã‚¹ãƒ†ãƒƒãƒ—)"
    )
    print(f"  KL Target: {model.kl_target}")
    print(f"  Free Bits: {model.free_bits}")

    # monitorãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    import os

    os.makedirs("monitor", exist_ok=True)
    os.makedirs("checkpoints", exist_ok=True)

    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    callbacks = [
        CollapseDetectionCallback(threshold=0.05, patience=5),
        ConditionMonitorCallback(test_pitch=60, check_every=10),
        tf.keras.callbacks.ModelCheckpoint(
            "checkpoints/epoch_{epoch:03d}.weights.h5",
            save_freq="epoch",
            save_weights_only=True,
            verbose=1,
        ),
        tf.keras.callbacks.ModelCheckpoint(
            "checkpoints/best_model.weights.h5",
            monitor="z_std_ema",
            mode="max",  # z_stdãŒå¤§ãã„æ–¹ãŒè‰¯ã„
            save_best_only=True,
            save_weights_only=True,
            verbose=1,
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="loss", factor=0.5, patience=15, min_lr=1e-6, verbose=1
        ),
        tf.keras.callbacks.CSVLogger("training_log.csv", append=True),
    ]

    # å­¦ç¿’å®Ÿè¡Œ
    print("\n[4] å­¦ç¿’é–‹å§‹...")
    print("=" * 60)

    history = model.fit(
        dataset,
        epochs=epochs,
        callbacks=callbacks,
        steps_per_epoch=steps_per_epoch,
        verbose=1,
    )

    print("\n" + "=" * 60)
    print("å­¦ç¿’å®Œäº†ï¼")
    print("=" * 60)

    # æœ€çµ‚è©•ä¾¡
    print("\n[5] æœ€çµ‚è©•ä¾¡:")
    final_logs = history.history
    print(f"  æœ€çµ‚ loss: {final_logs['loss'][-1]:.4f}")
    print(f"  æœ€çµ‚ z_std_ema: {final_logs['z_std_ema'][-1]:.4f}")
    print(f"  æœ€çµ‚ kl_weight: {final_logs['kl_weight'][-1]:.6f}")

    if final_logs["z_std_ema"][-1] > 0.1:
        print("\nâœ“ SUCCESS: æ½œåœ¨å¤‰æ•°ã¯å¥å…¨ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™")
    else:
        print("\nâš ï¸  WARNING: æ½œåœ¨å¤‰æ•°ã®æ´»ç”¨ãŒä¸ååˆ†ã§ã™")

    return model, history


if __name__ == "__main__":
    print("=" * 60)
    print("å¼·åŠ›ãªæ¡ä»¶ä»˜ã‘ãƒ¢ãƒ‡ãƒ« è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print("\nç‰¹å¾´:")
    print("  1. freq_feat ã‚’ä½¿ã‚ãªã„ï¼ˆæ¡ä»¶ã¨zã®ã¿ã§ç”Ÿæˆï¼‰")
    print("  2. TimbreEmbedding ã§éŸ³è‰²ã‚’ç‹¬ç«‹ã—ãŸç©ºé–“ã«")
    print("  3. StrongFiLM + Attention ã§æ¡ä»¶ã‚’å¼·åŠ›ã«åæ˜ ")
    print("  4. Free Bits + KL Annealing ã§Collapseé˜²æ­¢")
    print("  5. å®šæœŸçš„ãªæ¡ä»¶åˆ¥ç”Ÿæˆã§å­¦ç¿’ã‚’ç›£è¦–")
    print("=" * 60)

    # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    BATCH_SIZE = 16  # ãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´
    EPOCHS = 200

    print(f"\nãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE}")
    print(f"ã‚¨ãƒãƒƒã‚¯æ•°: {EPOCHS}")
    print("\nå­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/n)")
    # response = input().lower()

    # if response == 'y':
    model, history = train_with_strong_conditioning(
        batch_size=BATCH_SIZE, epochs=EPOCHS
    )

    print("\nâœ“ è¨“ç·´å®Œäº†")
    print("  é‡ã¿: checkpoints/best_model.weights.h5")
    print("  ãƒ­ã‚°: training_log.csv")
    print("  ç›£è¦–éŸ³å£°: monitor/epoch_*_*.wav")
